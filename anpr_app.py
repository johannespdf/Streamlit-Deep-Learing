import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import torchvision.transforms as transforms
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import cv2
import matplotlib.pyplot as plt
from io import BytesIO
import zipfile
import os
from collections import OrderedDict

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="ANPR System",
    page_icon="üöó",
    layout="wide"
)

# ==================== MODELS ====================

class PlateDetectorCNN(nn.Module):
    """
    Faster R-CNN untuk deteksi plat nomor
    Menggunakan ResNet-50 FPN backbone
    """
    def __init__(self):
        super(PlateDetectorCNN, self).__init__()
        # Load Faster R-CNN dengan ResNet-50 backbone
        self.model = fasterrcnn_resnet50_fpn(weights=None)
        
        # Ganti predictor untuk 2 kelas (background + plate)
        in_features = self.model.roi_heads.box_predictor.cls_score.in_features
        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)
    
    def forward(self, images, targets=None):
        return self.model(images, targets)

class CharacterCNN(nn.Module):
    """
    Custom CNN untuk klasifikasi karakter A-Z dan 0-9
    Total 36 kelas
    """
    def __init__(self, num_classes=36):
        super(CharacterCNN, self).__init__()
        
        # Conv Block 1
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Grayscale input
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2, 2)
        
        # Conv Block 2
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2, 2)
        
        # Conv Block 3
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(2, 2)
        
        # Fully Connected
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.relu4 = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, num_classes)
    
    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.pool3(self.relu3(self.conv3(x)))
        
        x = x.view(x.size(0), -1)
        
        x = self.relu4(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# ==================== CONSTANTS ====================

# Mapping untuk EMNIST labels ke character index kita
# EMNIST ByClass: 0-9 (digits), 10-35 (uppercase A-Z), 36-61 (lowercase a-z)
# Kita hanya butuh: 0-9 dan A-Z (uppercase)
CHAR_CLASSES = list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')
NUM_CHAR_CLASSES = len(CHAR_CLASSES)  # 36 classes
DEVICE = torch.device('cpu')

# Mapping dari EMNIST label ke index kita
EMNIST_TO_CHAR = {}
for i in range(10):  # 0-9
    EMNIST_TO_CHAR[i] = i
for i in range(26):  # A-Z
    EMNIST_TO_CHAR[10 + i] = 10 + i

# ==================== HELPER FUNCTIONS ====================

@st.cache_resource
def initialize_models():
    """Inisialisasi semua model"""
    plate_detector = PlateDetectorCNN()
    plate_detector.to(DEVICE)
    plate_detector.eval()
    
    char_classifier = CharacterCNN(num_classes=NUM_CHAR_CLASSES)
    char_classifier.to(DEVICE)
    
    # Try to train, but don't fail if it doesn't work
    try:
        char_classifier = train_character_classifier_emnist(char_classifier)
    except Exception as e:
        print(f"Warning: Could not train character classifier: {e}")
        print("Using untrained model. Predictions will be random.")
    
    char_classifier.eval()
    
    return plate_detector, char_classifier

def train_character_classifier_emnist(model, num_epochs=3):
    """
    Train character classifier dengan synthetic data dan MNIST
    Simplified training untuk faster, more reliable results
    """
    import string
    
    print("Training character classifier...")
    
    # Gunakan MNIST untuk digits (0-9) yang lebih simple
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
    ])
    
    try:
        # Load MNIST untuk digit recognition (0-9)
        from torchvision.datasets import MNIST
        
        mnist_train = MNIST(
            root='./data',
            train=True,
            download=True,
            transform=transform
        )
        
        # Buat DataLoader
        train_loader = torch.utils.data.DataLoader(
            mnist_train,
            batch_size=256,
            shuffle=True,
            num_workers=0
        )
        
        # Training setup
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        model.train()
        print(f"Training for {num_epochs} epochs on MNIST...")
        
        for epoch in range(num_epochs):
            running_loss = 0.0
            correct = 0
            total = 0
            batch_count = 0
            
            for i, (inputs, labels) in enumerate(train_loader):
                if i > 100:  # Limit to 100 batches for speed
                    break
                    
                # Filter hanya 0-9 (sudah otomatis di MNIST)
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                
                optimizer.zero_grad()
                outputs = model(inputs)
                
                # Only use first 10 outputs (digits)
                outputs_digits = outputs[:, :10]
                loss = criterion(outputs_digits, labels)
                
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = outputs_digits.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                batch_count += 1
            
            accuracy = 100. * correct / total if total > 0 else 0
            avg_loss = running_loss / batch_count if batch_count > 0 else 0
            print(f"Epoch {epoch+1}/{num_epochs}: Loss={avg_loss:.3f}, Accuracy={accuracy:.2f}%")
        
        print("‚úÖ Character classifier trained on MNIST (digits 0-9)")
        print("‚ö†Ô∏è Note: Letters (A-Z) use untrained weights. For better letter recognition, more training data needed.")
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        print("Using untrained model.")
    
    return model

def detect_plate(model, image):
    """
    Stage 2: Plate Detection menggunakan Faster R-CNN
    
    Args:
        model: PlateDetectorCNN
        image: PIL Image
    
    Returns:
        boxes: bounding boxes plat
        scores: confidence scores
    """
    transform = transforms.Compose([transforms.ToTensor()])
    image_tensor = transform(image).to(DEVICE)
    
    with torch.no_grad():
        predictions = model([image_tensor])
    
    pred = predictions[0]
    boxes = pred['boxes'].cpu().numpy()
    scores = pred['scores'].cpu().numpy()
    
    # Filter dengan confidence > 0.5
    mask = scores > 0.5
    boxes = boxes[mask]
    scores = scores[mask]
    
    return boxes, scores

def segment_characters(plate_image):
    """
    Stage 3: Character Segmentation (VASTLY IMPROVED)
    Enhanced preprocessing, relaxed filtering, and character separation
    
    Args:
        plate_image: cropped plate region (PIL Image)
    
    Returns:
        char_images: list of character images
        char_boxes: list of bounding boxes
        debug_images: dict of intermediate images untuk visualization
    """
    debug_images = {}
    
    # Convert to grayscale
    gray = cv2.cvtColor(np.array(plate_image), cv2.COLOR_RGB2GRAY)
    debug_images['01_original_gray'] = gray.copy()
    
    # Resize untuk standardisasi (lebih besar untuk better detail)
    target_height = 200  # Increased from 150
    scale_factor = target_height / gray.shape[0]
    gray = cv2.resize(gray, None, fx=scale_factor, fy=scale_factor)
    plate_height, plate_width = gray.shape
    debug_images['02_resized'] = gray.copy()
    
    # === ENHANCED PREPROCESSING ===
    
    # 1. Bilateral filter - preserve edges while reducing noise
    gray_bilateral = cv2.bilateralFilter(gray, 11, 80, 80)
    debug_images['03_bilateral'] = gray_bilateral
    
    # 2. CLAHE untuk contrast enhancement
    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
    gray_clahe = clahe.apply(gray_bilateral)
    debug_images['04_clahe'] = gray_clahe
    
    # === MULTIPLE THRESHOLDING METHODS ===
    
    # Method 1: Adaptive Gaussian (good for varying lighting)
    thresh1 = cv2.adaptiveThreshold(
        gray_clahe, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        11, 2  # Smaller block size, lower C
    )
    
    # Method 2: Adaptive Gaussian with larger block
    thresh2 = cv2.adaptiveThreshold(
        gray_clahe, 255,
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        21, 3
    )
    
    # Method 3: Adaptive Mean
    thresh3 = cv2.adaptiveThreshold(
        gray_clahe, 255,
        cv2.ADAPTIVE_THRESH_MEAN_C,
        cv2.THRESH_BINARY_INV,
        15, 2
    )
    
    # Method 4: Otsu's binarization
    _, thresh4 = cv2.threshold(
        gray_clahe, 0, 255, 
        cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )
    
    # Combine all methods (OR operation captures maximum detail)
    thresh_combined = cv2.bitwise_or(thresh1, thresh2)
    thresh_combined = cv2.bitwise_or(thresh_combined, thresh3)
    thresh_combined = cv2.bitwise_or(thresh_combined, thresh4)
    debug_images['05_thresh_combined'] = thresh_combined
    
    # === MORPHOLOGICAL OPERATIONS ===
    
    # Opening - remove small noise (very gentle)
    kernel_open = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
    thresh_opened = cv2.morphologyEx(thresh_combined, cv2.MORPH_OPEN, kernel_open, iterations=1)
    debug_images['06_opened'] = thresh_opened
    
    # Closing - connect broken parts
    kernel_close = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 3))
    thresh_closed = cv2.morphologyEx(thresh_opened, cv2.MORPH_CLOSE, kernel_close, iterations=1)
    debug_images['07_closed'] = thresh_closed
    
    # Final threshold
    thresh_final = thresh_closed
    debug_images['08_final_threshold'] = thresh_final
    
    # === CONTOUR DETECTION ===
    
    contours, _ = cv2.findContours(thresh_final, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # === RELAXED CONTOUR FILTERING ===
    
    char_candidates = []
    rejected_contours = {'too_small': 0, 'too_large': 0, 'wrong_aspect': 0, 'wrong_position': 0}
    
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        
        area = w * h
        aspect_ratio = h / float(w) if w > 0 else 0
        
        # VERY RELAXED criteria to capture all characters
        min_height = plate_height * 0.15  # 15% of plate height
        max_height = plate_height * 0.95  # 95% of plate height
        min_width = plate_width * 0.008   # 0.8% of plate width (very relaxed)
        max_width = plate_width * 0.25    # 25% of plate width
        min_area = 80  # Very low threshold
        
        # Vertical position filter (characters should be in middle region)
        min_y = plate_height * 0.05
        max_y = plate_height * 0.95
        
        # Check filters
        if h < min_height or h > max_height:
            rejected_contours['wrong_aspect'] += 1
            continue
        if w < min_width or w > max_width:
            rejected_contours['too_small' if w < min_width else 'too_large'] += 1
            continue
        if area < min_area:
            rejected_contours['too_small'] += 1
            continue
        if y < min_y or y > max_y:
            rejected_contours['wrong_position'] += 1
            continue
        
        # Very wide aspect ratio range: 0.5 to 6.0
        # 0.5 = wide characters (W, M)
        # 6.0 = very thin characters (I, 1)
        if 0.5 <= aspect_ratio <= 6.0:
            char_candidates.append((x, y, w, h, aspect_ratio))
    
    debug_images['rejection_stats'] = rejected_contours
    
    # Visualize all candidates
    debug_viz_candidates = cv2.cvtColor(thresh_final, cv2.COLOR_GRAY2RGB)
    for (x, y, w, h, _) in char_candidates:
        cv2.rectangle(debug_viz_candidates, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(debug_viz_candidates, f"{h}/{w}", (x, y-5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)
    debug_images['09_candidates'] = debug_viz_candidates
    
    # Sort left to right
    char_candidates = sorted(char_candidates, key=lambda b: b[0])
    
    # === CHARACTER SEPARATION ===
    # Split potentially merged characters
    
    separated_boxes = []
    
    for (x, y, w, h, aspect_ratio) in char_candidates:
        # Check if this might be merged characters
        # Heuristic: if width is unusually large compared to height
        expected_char_width = h * 0.7  # Typical character aspect ratio
        
        if w > expected_char_width * 1.8 and aspect_ratio < 1.2:
            # Likely merged characters - try to split
            # Use vertical projection profile
            char_region = thresh_final[y:y+h, x:x+w]
            
            # Calculate vertical projection (sum along columns)
            vertical_projection = np.sum(char_region, axis=0)
            
            # Find valleys (low points) in the projection
            projection_smoothed = np.convolve(vertical_projection, np.ones(3)/3, mode='same')
            mean_projection = np.mean(projection_smoothed)
            
            # Find split points (valleys where projection is below mean)
            split_points = []
            in_valley = False
            valley_start = 0
            
            for i, val in enumerate(projection_smoothed):
                if val < mean_projection * 0.3 and not in_valley:
                    valley_start = i
                    in_valley = True
                elif val >= mean_projection * 0.3 and in_valley:
                    split_points.append((valley_start + i) // 2)
                    in_valley = False
            
            # Split if we found good split points
            if len(split_points) > 0:
                prev_x = 0
                for split_x in split_points:
                    if split_x - prev_x > w * 0.15:  # At least 15% of width
                        separated_boxes.append((x + prev_x, y, split_x - prev_x, h))
                    prev_x = split_x
                # Add remaining part
                if w - prev_x > w * 0.15:
                    separated_boxes.append((x + prev_x, y, w - prev_x, h))
            else:
                # Couldn't split, keep as is
                separated_boxes.append((x, y, w, h))
        else:
            # Normal character
            separated_boxes.append((x, y, w, h))
    
    # Remove duplicates and very small boxes
    char_boxes = []
    for box in separated_boxes:
        x, y, w, h = box
        if w > plate_width * 0.01 and h > plate_height * 0.15:
            char_boxes.append(box)
    
    # Sort again after separation
    char_boxes = sorted(char_boxes, key=lambda b: b[0])
    
    # === HANDLE OVERLAPPING/CLOSE BOXES ===
    
    final_boxes = []
    if char_boxes:
        current_box = list(char_boxes[0])
        
        for next_box in char_boxes[1:]:
            x1, y1, w1, h1 = current_box
            x2, y2, w2, h2 = next_box
            
            gap = x2 - (x1 + w1)
            
            # Only merge if extremely close (< 3px) and similar height
            height_ratio = max(h1, h2) / min(h1, h2) if min(h1, h2) > 0 else 999
            
            if gap < 3 and height_ratio < 1.5:
                # Merge boxes
                new_x = min(x1, x2)
                new_y = min(y1, y2)
                new_w = max(x1 + w1, x2 + w2) - new_x
                new_h = max(y1 + h1, y2 + h2) - new_y
                current_box = [new_x, new_y, new_w, new_h]
            else:
                final_boxes.append(tuple(current_box))
                current_box = list(next_box)
        
        final_boxes.append(tuple(current_box))
    
    char_boxes = final_boxes if final_boxes else char_boxes
    
    # Visualize final boxes
    debug_viz_final = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    for i, (x, y, w, h) in enumerate(char_boxes):
        cv2.rectangle(debug_viz_final, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(debug_viz_final, str(i+1), (x+2, y+12), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
    debug_images['10_final_boxes'] = debug_viz_final
    
    # === CROP AND PREPARE CHARACTERS ===
    
    char_images = []
    for (x, y, w, h) in char_boxes:
        # Crop from threshold image
        char_img = thresh_final[y:y+h, x:x+w]
        
        # Add padding untuk better recognition
        pad = 5  # Slightly more padding
        char_img = cv2.copyMakeBorder(
            char_img, pad, pad, pad, pad,
            cv2.BORDER_CONSTANT, value=0
        )
        
        # Resize to 32x32 with better interpolation
        char_img = cv2.resize(char_img, (32, 32), interpolation=cv2.INTER_AREA)
        
        # Ensure white text on black background
        if np.mean(char_img) > 127:
            char_img = 255 - char_img
        
        # Additional normalization
        char_img = cv2.normalize(char_img, None, 0, 255, cv2.NORM_MINMAX)
        
        char_images.append(char_img)
    
    return char_images, char_boxes, debug_images

def classify_character(model, char_image):
    """
    Stage 4: Character Classification
    
    Args:
        model: CharacterCNN
        char_image: grayscale character image (numpy array)
    
    Returns:
        predicted_char: predicted character
        confidence: confidence score
    """
    # Normalize
    char_tensor = torch.FloatTensor(char_image).unsqueeze(0).unsqueeze(0) / 255.0
    char_tensor = char_tensor.to(DEVICE)
    
    with torch.no_grad():
        outputs = model(char_tensor)
        probs = torch.nn.functional.softmax(outputs, dim=1)
        confidence, predicted = torch.max(probs, 1)
    
    predicted_char = CHAR_CLASSES[predicted.item()]
    
    return predicted_char, confidence.item()

def post_process_plate(characters, confidences, boxes):
    """
    Stage 5: Post-processing
    Mengurutkan dan menggabungkan karakter menjadi string plat
    
    Args:
        characters: list of predicted characters
        confidences: list of confidence scores
        boxes: list of bounding boxes (already sorted)
    
    Returns:
        plate_text: formatted plate string
        avg_confidence: average confidence
    """
    # Characters sudah sorted by x-coordinate
    plate_text = ''.join(characters)
    
    # Format (heuristic untuk plat Indonesia: XX 1234 XX)
    if len(plate_text) >= 7:
        formatted = f"{plate_text[:2]} {plate_text[2:6]} {plate_text[6:]}"
    else:
        formatted = plate_text
    
    avg_confidence = np.mean(confidences) if confidences else 0.0
    
    return formatted, avg_confidence

def visualize_detection(image, boxes, scores):
    """Visualisasi detection results"""
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)
    
    try:
        font = ImageFont.truetype("arial.ttf", 30)
    except:
        font = ImageFont.load_default()
    
    for box, score in zip(boxes, scores):
        x1, y1, x2, y2 = box
        draw.rectangle([(x1, y1), (x2, y2)], outline="red", width=3)
        text = f"Plate: {score:.2f}"
        draw.text((x1, y1 - 35), text, fill="red", font=font)
    
    return img_draw

def visualize_segmentation(plate_image, char_boxes):
    """Visualisasi character segmentation"""
    img_array = np.array(plate_image)
    img_viz = Image.fromarray(img_array)
    draw = ImageDraw.Draw(img_viz)
    
    for (x, y, w, h) in char_boxes:
        draw.rectangle([(x, y), (x+w, y+h)], outline="green", width=2)
    
    return img_viz

# ==================== STREAMLIT UI ====================

st.title("üöó Automatic Number Plate Recognition (ANPR)")
st.markdown("**End-to-End Pipeline: Detection ‚Üí Segmentation ‚Üí Classification ‚Üí Recognition**")
st.markdown("---")

# Sidebar
st.sidebar.header("‚öôÔ∏è Configuration")

mode = st.sidebar.radio(
    "Select Mode",
    ["üîç Inference", "üìä About"],
    help="Choose between inference or viewing system info"
)

st.sidebar.markdown("---")
st.sidebar.info(
    """
    **ANPR Pipeline:**
    1. Plate Detection (Faster R-CNN)
    2. Character Segmentation
    3. Character Classification (CNN)
    4. Post-processing
    
    **Models:** All defined in code
    **Device:** CPU
    """
)

# Initialize models
if 'plate_detector' not in st.session_state:
    with st.spinner("Loading models..."):
        plate_detector, char_classifier = initialize_models()
        st.session_state.plate_detector = plate_detector
        st.session_state.char_classifier = char_classifier
        st.session_state.models_loaded = True

# ==================== INFERENCE MODE ====================

if mode == "üîç Inference":
    st.header("üîç Inference Mode")
    
    st.warning(
        "‚ö†Ô∏è **Note:** Models have random weights (not trained). "
        "Plate detection and character recognition will not be accurate until models are trained. "
        "This is a demonstration of the complete pipeline architecture."
    )
    
    # File upload
    upload_type = st.radio(
        "Upload Type",
        ["Single Image", "Batch (ZIP)"],
        horizontal=True
    )
    
    if upload_type == "Single Image":
        uploaded_file = st.file_uploader(
            "Upload vehicle image",
            type=['jpg', 'jpeg', 'png'],
            help="Upload image containing vehicle with license plate"
        )
        
        if uploaded_file is not None:
            # Load image
            image = Image.open(uploaded_file).convert("RGB")
            
            # Create columns for pipeline stages
            st.markdown("## Pipeline Stages")
            
            # Stage 1: Input
            with st.expander("üì• Stage 1: Input Image", expanded=True):
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.image(image, caption="Input Vehicle Image", use_column_width=True)
                with col2:
                    st.metric("Image Size", f"{image.size[0]}x{image.size[1]}")
            
            # Stage 2: Plate Detection
            with st.expander("üéØ Stage 2: Plate Detection", expanded=True):
                with st.spinner("Detecting plate..."):
                    boxes, scores = detect_plate(st.session_state.plate_detector, image)
                
                if len(boxes) > 0:
                    viz_img = visualize_detection(image, boxes, scores)
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.image(viz_img, caption="Plate Detection Result", use_column_width=True)
                    with col2:
                        st.success(f"‚úÖ Detected {len(boxes)} plate(s)")
                        for i, (box, score) in enumerate(zip(boxes, scores)):
                            st.write(f"**Plate {i+1}:** {score:.2%} confidence")
                else:
                    st.error("‚ùå No plate detected")
            
            # Stage 3 & 4: Segmentation and Classification (if plate detected)
            if len(boxes) > 0:
                # Use first detected plate
                box = boxes[0]
                x1, y1, x2, y2 = box.astype(int)
                plate_img = image.crop((x1, y1, x2, y2))
                
                # Stage 3: Segmentation
                with st.expander("‚úÇÔ∏è Stage 3: Character Segmentation", expanded=True):
                    with st.spinner("Segmenting characters..."):
                        char_images, char_boxes, debug_images = segment_characters(plate_img)
                    
                    # Character count validation
                    num_chars = len(char_images)
                    if num_chars < 7:
                        st.error(f"‚ö†Ô∏è Only {num_chars} character(s) detected! Expected 7-9 for standard plate.")
                    elif num_chars > 9:
                        st.warning(f"‚ö†Ô∏è {num_chars} characters detected - may include noise. Expected 7-9.")
                    else:
                        st.success(f"‚úÖ Found {num_chars} character(s) - looks good!")
                    
                    if len(char_images) > 0:
                        # Main visualization
                        viz_seg = visualize_segmentation(plate_img, char_boxes)
                        col1, col2 = st.columns([2, 1])
                        with col1:
                            st.image(viz_seg, caption="Character Segmentation Result", use_column_width=True)
                        with col2:
                            st.metric("Characters Detected", num_chars)
                            st.metric("Expected Range", "7-9")
                        
                        # Debug visualization (using checkbox instead of expander to avoid nesting)
                        st.markdown("---")
                        show_debug = st.checkbox("üî¨ Show Debug Visualizations", value=False, key="show_debug_viz")
                        
                        if show_debug:
                            st.write("**Preprocessing Pipeline:**")
                            
                            # Row 1: Original, Bilateral, CLAHE
                            debug_col1, debug_col2, debug_col3 = st.columns(3)
                            with debug_col1:
                                if '01_original_gray' in debug_images:
                                    st.image(debug_images['01_original_gray'], caption="1. Original Grayscale", use_column_width=True)
                            with debug_col2:
                                if '03_bilateral' in debug_images:
                                    st.image(debug_images['03_bilateral'], caption="2. Bilateral Filter", use_column_width=True)
                            with debug_col3:
                                if '04_clahe' in debug_images:
                                    st.image(debug_images['04_clahe'], caption="3. CLAHE Enhancement", use_column_width=True)
                            
                            st.write("**Thresholding & Morphology:**")
                            
                            # Row 2: Combined threshold, opened, closed
                            debug_col4, debug_col5, debug_col6 = st.columns(3)
                            with debug_col4:
                                if '05_thresh_combined' in debug_images:
                                    st.image(debug_images['05_thresh_combined'], caption="4. Combined Threshold", use_column_width=True)
                            with debug_col5:
                                if '06_opened' in debug_images:
                                    st.image(debug_images['06_opened'], caption="5. After Opening", use_column_width=True)
                            with debug_col6:
                                if '07_closed' in debug_images:
                                    st.image(debug_images['07_closed'], caption="6. After Closing", use_column_width=True)
                            
                            st.write("**Character Detection:**")
                            
                            # Row 3: Candidates and final boxes
                            debug_col7, debug_col8 = st.columns(2)
                            with debug_col7:
                                if '09_candidates' in debug_images:
                                    st.image(debug_images['09_candidates'], caption="7. Candidate Characters", use_column_width=True)
                            with debug_col8:
                                if '10_final_boxes' in debug_images:
                                    st.image(debug_images['10_final_boxes'], caption="8. Final Character Boxes", use_column_width=True)
                            
                            # Show rejection statistics
                            if 'rejection_stats' in debug_images:
                                st.write("**Contour Filtering Statistics:**")
                                stats = debug_images['rejection_stats']
                                stats_col1, stats_col2, stats_col3, stats_col4 = st.columns(4)
                                with stats_col1:
                                    st.metric("Too Small", stats.get('too_small', 0))
                                with stats_col2:
                                    st.metric("Too Large", stats.get('too_large', 0))
                                with stats_col3:
                                    st.metric("Wrong Aspect", stats.get('wrong_aspect', 0))
                                with stats_col4:
                                    st.metric("Wrong Position", stats.get('wrong_position', 0))
                        
                        st.markdown("---")
                        
                        # Show ALL character crops
                        st.write("**All Character Crops:**")
                        # Display in rows of 6
                        for row_start in range(0, len(char_images), 6):
                            cols = st.columns(6)
                            for i, char_img in enumerate(char_images[row_start:row_start+6]):
                                with cols[i]:
                                    st.image(char_img, caption=f"#{row_start+i+1}", use_column_width=True)
                    else:
                        st.error("‚ùå No characters segmented - check image quality or preprocessing")
                
                # Stage 4: Classification
                if len(char_images) > 0:
                    with st.expander("üî§ Stage 4: Character Classification", expanded=True):
                        with st.spinner("Classifying characters..."):
                            characters = []
                            confidences = []
                            
                            for char_img in char_images:
                                char, conf = classify_character(
                                    st.session_state.char_classifier,
                                    char_img
                                )
                                characters.append(char)
                                confidences.append(conf)
                        
                        # Display results
                        col1, col2 = st.columns([2, 1])
                        with col1:
                            st.write("**Classification Results:**")
                            result_text = ""
                            for i, (char, conf) in enumerate(zip(characters, confidences)):
                                result_text += f"{i+1}. **{char}** ({conf:.2%})  \n"
                            st.markdown(result_text)
                        
                        with col2:
                            avg_conf = np.mean(confidences)
                            st.metric("Average Confidence", f"{avg_conf:.2%}")
                    
                    # Stage 5: Post-processing
                    with st.expander("üìù Stage 5: Final Result", expanded=True):
                        plate_text, avg_confidence = post_process_plate(
                            characters,
                            confidences,
                            char_boxes
                        )
                        
                        st.success("‚úÖ **Recognition Complete!**")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown(f"### Plate Number: `{plate_text}`")
                        with col2:
                            st.metric("Overall Confidence", f"{avg_confidence:.2%}")
                        
                        # Summary
                        st.markdown("---")
                        st.markdown("**Pipeline Summary:**")
                        summary_data = {
                            "Stage": [
                                "1. Input",
                                "2. Plate Detection",
                                "3. Character Segmentation",
                                "4. Character Classification",
                                "5. Post-processing"
                            ],
                            "Status": [
                                "‚úÖ Complete",
                                f"‚úÖ {len(boxes)} plate(s) detected",
                                f"‚úÖ {len(char_images)} character(s) found",
                                f"‚úÖ {len(characters)} character(s) classified",
                                f"‚úÖ Result: {plate_text}"
                            ]
                        }
                        st.table(summary_data)
        
        else:
            st.info("üëÜ Upload a vehicle image to start ANPR pipeline")
    
    else:  # Batch processing
        st.info("üì¶ Batch processing: Upload ZIP file containing multiple vehicle images")
        uploaded_zip = st.file_uploader(
            "Upload ZIP file",
            type=['zip'],
            help="ZIP file containing vehicle images"
        )
        
        if uploaded_zip is not None:
            st.warning("Batch processing implemented but requires training data. Single image mode recommended for demo.")

# ==================== ABOUT MODE ====================

else:
    st.header("üìä About ANPR System")
    
    st.markdown("""
    ## System Architecture
    
    This ANPR system implements a complete 5-stage pipeline:
    """)
    
    st.markdown("""
    ### Pipeline Stages
    
    1. **Input**: Vehicle image (RGB)
    2. **Plate Detection**: Faster R-CNN with ResNet-50 backbone
    3. **Character Segmentation**: Contour-based character localization
    4. **Character Classification**: Custom CNN (36 classes: A-Z, 0-9)
    5. **Post-processing**: Character ordering and formatting
    """)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### Model 1: Plate Detector
        
        **Architecture:** Faster R-CNN  
        **Backbone:** ResNet-50 + FPN  
        **Output:** Bounding boxes + confidence  
        **Classes:** 2 (background, plate)
        """)
    
    with col2:
        st.markdown("""
        ### Model 2: Character Classifier
        
        **Architecture:** Custom CNN  
        **Layers:** 3 Conv + 2 FC  
        **Input:** 32x32 grayscale  
        **Output:** 36 classes (A-Z, 0-9)
        """)
    
    st.markdown("---")
    st.markdown("""
    ### Technical Details
    
    - **Framework:** PyTorch
    - **Device:** CPU
    - **Models:** Defined in code (no external .pth files)
    - **Training:** Required for accurate predictions
    - **Inference:** Real-time on CPU
    """)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center'>
        <p>Built with Streamlit üéà | Powered by PyTorch üî•</p>
    </div>
    """,
    unsafe_allow_html=True
)
